<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Publications | Xufeng Zhao / ËµµÁª™Èîã</title> <meta name="author" content="Xufeng Zhao / ËµµÁª™Èîã"> <meta name="description" content="üïò 3rd year Ph.D student in University of Hamburg; Researching on reinforcement learning (RL), robotic, and large language models (LLMs). "> <meta name="keywords" content="Robotics, LLMs, Reinforcement Learning"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/squirtle_circle_transparent.png"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://xf-zhao.github.io/publications/"> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><img src="/assets/img/squirtle_circle_transparent.png" width="auto" height="35">¬†<span class="font-weight-bold">Xufeng¬†</span>Zhao / ËµµÁª™Èîã</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Posts</a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">Publications<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/code/">Code</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> <li class="nav-item "> <a class="nav-link" href="/contact/">Contact</a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Publications</h1> <p class="post-description"></p> </header> <article> <p>My interests are in a <ins>‚Äúresearch triangle‚Äù</ins>: <img style="float:right;" src="/assets/img/research_triangle.png" width="350" height="auto"></p> <ul> <li> <strong>Large language models</strong> have high capacities to reason universally</li> <li> <strong>Reinforcement learning</strong> optimize agent behavior to maximize expectations</li> <li> <strong>Robots</strong> embody the intellegence to our real world</li> </ul> <div class="publications"> <em>in reversed chronological order / <b>*</b>equal contributions / <b><font color="#FC8EAC">featured works</font></b></em> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Lafite-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Lafite-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Lafite-1400.webp"></source> <img src="/assets/img/publication_preview/Lafite.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Lafite.jpg" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="chu2023accelerating" class="col-sm-8"> <div class="title">Accelerating Reinforcement Learning of Robotic Manipulations via Feedback from Large Language Models</div> <div class="author"> <a href="https://kchu.github.io/" style="color: gray" rel="external nofollow noopener" target="_blank">Kun Chu</a>,¬†<strong>Xufeng Zhao</strong>,¬†<a href="https://www.inf.uni-hamburg.de/en/inst/ab/wtm/people/weber.html" style="color: gray" rel="external nofollow noopener" target="_blank">Cornelius Weber</a>,¬†<a href="https://mengdi-li.github.io" style="color: gray" rel="external nofollow noopener" target="_blank">Mengdi Li</a>,¬†and¬†<a href="https://www.inf.uni-hamburg.de/en/inst/ab/wtm/people/wermter.html" style="color: gray" rel="external nofollow noopener" target="_blank">Stefan Wermter</a> </div> <div class="periodical"> <em>In 7th Conference on Robot Learning (CoRL 2023) Workshop (<p style="color:red;display:inline"><b>oral</b></p>)</em>, Nov 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract ¬†<img src="/assets/img/arrowdown-icon.png" height="4"></a> <a href="http://arxiv.org/abs/2311.02379" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><img src="/assets/img/arxiv-logo.png" height="16"></a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-arxiv-id="2311.02379"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Reinforcement Learning (RL) plays an important role in the robotic manipulation domain since it allows self-learning from trial-and-error interactions with the environment. Still, sample efficiency and reward specification seriously limit its potential. One possible solution involves learning from expert guidance. However, obtaining a human expert is impractical due to the high cost of supervising an RL agent, and developing an automatic supervisor is a challenging endeavor. Large Language Models (LLMs) demonstrate remarkable abilities to provide human-like feedback on user inputs in natural language. Nevertheless, they are not designed to directly control low-level robotic motions, as their pretraining is based on vast internet data rather than specific robotics data. In this paper, we introduce the Lafite-RL (Language agent feedback interactive Reinforcement Learning) framework, which enables RL agents to learn robotic tasks efficiently by taking advantage of LLMs‚Äô timely feedback. Our experiments conducted on RLBench tasks illustrate that, with simple prompt design in natural language, the Lafite-RL agent exhibits improved learning capabilities when guided by an LLM. It outperforms the baseline in terms of both learning efficiency and success rate, underscoring the efficacy of the rewards provided by an LLM.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"><img data-zoomable="" class="preview z-depth-1 rounded" src="https://logi-cot.github.io/img/LogiCoT_preview.gif"></div> <div id="Zhao23EnhancingZeroShot" class="col-sm-8"> <div class="title">Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models through Logic</div> <div class="author"> <strong>Xufeng Zhao</strong>,¬†<a href="https://mengdi-li.github.io" style="color: gray" rel="external nofollow noopener" target="_blank">Mengdi Li</a>,¬†<a href="https://www.inf.uni-hamburg.de/en/inst/ab/wtm/people/lu.html" style="color: gray" rel="external nofollow noopener" target="_blank">Wenhao Lu</a>,¬†<a href="https://www.inf.uni-hamburg.de/en/inst/ab/wtm/people/weber.html" style="color: gray" rel="external nofollow noopener" target="_blank">Cornelius Weber</a>,¬†<a href="https://jaeheelee.gitlab.io/" style="color: gray" rel="external nofollow noopener" target="_blank">Jae Hee Lee</a>,¬†<a href="https://kchu.github.io/" style="color: gray" rel="external nofollow noopener" target="_blank">Kun Chu</a>,¬†and¬†<a href="https://www.inf.uni-hamburg.de/en/inst/ab/wtm/people/wermter.html" style="color: gray" rel="external nofollow noopener" target="_blank">Stefan Wermter</a> </div> <div class="periodical"> <em>In arXiv preprint</em>, Sep 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract ¬†<img src="/assets/img/arrowdown-icon.png" height="4"></a> <a href="http://arxiv.org/abs/2309.13339" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><img src="/assets/img/arxiv-logo.png" height="16"></a> <a href="https://logi-cot.github.io/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-arxiv-id="2309.13339"></span> <span class="__dimensions_badge_embed__" data-doi="10.48550/arXiv.2309.13339" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Recent advancements in large language models have showcased their remarkable generalizability across various domains. However, their reasoning abilities still have significant room for improvement, especially when confronted with scenarios requiring multi-step reasoning. Although large language models possess extensive knowledge, their behavior, particularly in terms of reasoning, often fails to effectively utilize this knowledge to establish a coherent thinking paradigm. Generative language models sometimes show hallucinations as their reasoning procedures are unconstrained by logical principles. Aiming to improve the zero-shot chain-of-thought reasoning ability of large language models, we propose Logical Chain-of-Thought (LogiCoT), a neurosymbolic framework that leverages principles from symbolic logic to verify and revise the reasoning processes accordingly. Experimental evaluations conducted on language tasks in diverse domains, including arithmetic, commonsense, symbolic, causal inference, and social problems, demonstrate the efficacy of the enhanced reasoning paradigm by logic.</p> </div> </div> </div> </li> <li> <div class="row" style="background-image: linear-gradient(to right, rgba(255,0,0,0.005), rgba(255,0,0,.05));"> <div class="col-sm-2 preview"><img data-zoomable="" class="preview z-depth-1 rounded" src="https://matcha-agent.github.io/img/Matcha_preview.gif"></div> <div id="Zhao23ChatEnvironment" class="col-sm-8"> <div class="title">Chat with the Environment: Interactive Multimodal Perception Using Large Language Models</div> <div class="author"> <strong>Xufeng Zhao</strong>,¬†<a href="https://mengdi-li.github.io" style="color: gray" rel="external nofollow noopener" target="_blank">Mengdi Li</a>,¬†<a href="https://www.inf.uni-hamburg.de/en/inst/ab/wtm/people/weber.html" style="color: gray" rel="external nofollow noopener" target="_blank">Cornelius Weber</a>,¬†<a href="https://www.mbhafez.com/" style="color: gray" rel="external nofollow noopener" target="_blank">Muhammad Burhan Hafez</a>,¬†and¬†<a href="https://www.inf.uni-hamburg.de/en/inst/ab/wtm/people/wermter.html" style="color: gray" rel="external nofollow noopener" target="_blank">Stefan Wermter</a> </div> <div class="periodical"> <em>In 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, Oct 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract ¬†<img src="/assets/img/arrowdown-icon.png" height="4"></a> <a href="http://arxiv.org/abs/2303.08268" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><img src="/assets/img/arxiv-logo.png" height="16"></a> <a href="https://github.com/xf-zhao/Matcha" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://matcha-agent.github.io/img/matcha-poster.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Poster</a> <a href="https://docs.google.com/presentation/d/1ks5GJsmXNNILMLPINUQTXiw6NI98rc8z/edit?usp=sharing&amp;ouid=116416476187922393030&amp;rtpof=true&amp;sd=true" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Slides</a> <a href="https://matcha-agent.github.io/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-arxiv-id="2303.08268"></span> <span class="__dimensions_badge_embed__" data-doi="10.48550/arXiv.2303.08268" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Programming robot behavior in a complex world faces challenges on multiple levels, from dextrous low-level skills to high-level planning and reasoning. Recent pre-trained Large Language Models (LLMs) have shown remarkable reasoning ability in few-shot robotic planning. However, it remains challenging to ground LLMs in multimodal sensory input and continuous action output, while enabling a robot to interact with its environment and acquire novel information as its policies unfold. We develop a robot interaction scenario with a partially observable state, which necessitates a robot to decide on a range of epistemic actions in order to sample sensory information among multiple modalities, before being able to execute the task correctly. Matcha (Multimodal environment chatting) agent, an interactive perception framework, is therefore proposed with an LLM as its backbone, whose ability is exploited to instruct epistemic actions and to reason over the resulting multimodal sensations (vision, sound, haptics, proprioception), as well as to plan an entire task execution based on the interactively acquired information. Our study demonstrates that LLMs can provide high-level planning and reasoning skills and control interactive robot behavior in a multimodal environment, while multimodal modules with the context of the environmental state help ground the LLMs and extend their processing ability. The project website can be found at https://matcha-agent.github.io/.</p> </div> </div> </div> </li> <li> <div class="row" style="background-image: linear-gradient(to right, rgba(255,0,0,0.005), rgba(255,0,0,.05));"> <div class="col-sm-2 preview"><img data-zoomable="" class="preview z-depth-1 rounded" src="https://xf-zhao.github.io/assets/img/publication_preview/IRRL_teaser.gif"></div> <div id="Li*23InternallyRewarded" class="col-sm-8"> <div class="title">Internally Rewarded Reinforcement Learning</div> <div class="author"> <a href="https://mengdi-li.github.io" style="color: gray" rel="external nofollow noopener" target="_blank">Mengdi Li*</a>,¬†<strong>Xufeng Zhao*</strong>,¬†<a href="https://jaeheelee.gitlab.io/" style="color: gray" rel="external nofollow noopener" target="_blank">Jae Hee Lee</a>,¬†<a href="https://www.inf.uni-hamburg.de/en/inst/ab/wtm/people/weber.html" style="color: gray" rel="external nofollow noopener" target="_blank">Cornelius Weber</a>,¬†and¬†<a href="https://www.inf.uni-hamburg.de/en/inst/ab/wtm/people/wermter.html" style="color: gray" rel="external nofollow noopener" target="_blank">Stefan Wermter</a> </div> <div class="periodical"> <em>In 40th International Conference on Machine Learning (ICML)</em>, Jul 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract ¬†<img src="/assets/img/arrowdown-icon.png" height="4"></a> <a href="http://arxiv.org/abs/2302.00270" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><img src="/assets/img/arxiv-logo.png" height="16"></a> <a href="https://github.com/mengdi-li/internally-rewarded-rl" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://ir-rl.github.io/img/ICML-poster.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Poster</a> <a href="https://ir-rl.github.io/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-arxiv-id="2302.00270"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>We study a class of reinforcement learning problems where the reward signals for policy learning are generated by a discriminator that is dependent on and jointly optimized with the policy. This interdependence between the policy and the discriminator leads to an unstable learning process because reward signals from an immature discriminator are noisy and impede policy learning, and conversely, an under-optimized policy impedes discriminator learning. We call this learning setting \textit{Internally Rewarded Reinforcement Learning} (IRRL) as the reward is not provided directly by the environment but \textit{internally} by the discriminator. In this paper, we formally formulate IRRL and present a class of problems that belong to IRRL. We theoretically derive and empirically analyze the effect of the reward function in IRRL and based on these analyses propose the clipped linear reward function. Experimental results show that the proposed reward function can consistently stabilize the training process by reducing the impact of reward noise, which leads to faster convergence and higher performance compared with baselines in diverse tasks.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#318CE7"><a href="https://www.icdl-2023.org/" rel="external nofollow noopener" target="_blank">IEEE ICDL 2023</a></abbr></div> <div id="Lu23CloserLook" class="col-sm-8"> <div class="title">A Closer Look at Reward Decomposition for High-Level Robotic Explanations</div> <div class="author"> <a href="https://www.inf.uni-hamburg.de/en/inst/ab/wtm/people/lu.html" style="color: gray" rel="external nofollow noopener" target="_blank">Wenhao Lu</a>,¬†<strong>Xufeng Zhao</strong>,¬†<a href="https://www.inf.uni-hamburg.de/en/inst/ab/wtm/people/magg.html" style="color: gray" rel="external nofollow noopener" target="_blank">Sven Magg</a>,¬†Martin Gromniak,¬†and¬†<a href="https://www.inf.uni-hamburg.de/en/inst/ab/wtm/people/wermter.html" style="color: gray" rel="external nofollow noopener" target="_blank">Stefan Wermter</a> </div> <div class="periodical"> <em>In IEEE International Conference on Development and Learning (ICDL)</em>, Nov 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract ¬†<img src="/assets/img/arrowdown-icon.png" height="4"></a> <a href="http://arxiv.org/abs/2304.12958" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><img src="/assets/img/arxiv-logo.png" height="16"></a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-arxiv-id="2304.12958"></span> <span class="__dimensions_badge_embed__" data-doi="10.48550/arXiv.2304.12958" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Explaining the behavior of intelligent agents such as robots to humans is challenging due to their incomprehensible proprioceptive states, variational intermediate goals, and resultant unpredictability. Moreover, one-step explanations for reinforcement learning agents can be ambiguous as they fail to account for the agent‚Äôs future behavior at each transition, adding to the complexity of explaining robot actions. By leveraging abstracted actions that map to task-specific primitives, we avoid explanations on the movement level. Our proposed framework combines reward decomposition (RD) with abstracted action spaces into an explainable learning framework, allowing for non-ambiguous and high-level explanations based on object properties in the task. We demonstrate the effectiveness of our framework through quantitative and qualitative analysis of two robot scenarios, showcasing visual and textual explanations, from output artifacts of RD explanation, that are easy for humans to comprehend. Additionally, we demonstrate the versatility of integrating these artifacts with large language models for reasoning and interactive querying.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"><li> <div class="row" style="background-image: linear-gradient(to right, rgba(255,0,0,0.005), rgba(255,0,0,.05));"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/ISCM_teaser-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/ISCM_teaser-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/ISCM_teaser-1400.webp"></source> <img src="/assets/img/publication_preview/ISCM_teaser.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="ISCM_teaser.jpg" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Zhao22ImpactMakes" class="col-sm-8"> <div class="title">Impact Makes a Sound and Sound Makes an Impact: Sound Guides Representations and Explorations</div> <div class="author"> <strong>Xufeng Zhao</strong>,¬†<a href="https://www.inf.uni-hamburg.de/en/inst/ab/wtm/people/weber.html" style="color: gray" rel="external nofollow noopener" target="_blank">Cornelius Weber</a>,¬†<a href="https://www.mbhafez.com/" style="color: gray" rel="external nofollow noopener" target="_blank">Muhammad Burhan Hafez</a>,¬†and¬†<a href="https://www.inf.uni-hamburg.de/en/inst/ab/wtm/people/wermter.html" style="color: gray" rel="external nofollow noopener" target="_blank">Stefan Wermter</a> </div> <div class="periodical"> <em>In 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, Oct 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract ¬†<img src="/assets/img/arrowdown-icon.png" height="4"></a> <a href="http://arxiv.org/abs/2208.02680" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><img src="/assets/img/arxiv-logo.png" height="16"></a> <a href="news/IROS2022/" class="btn btn-sm z-depth-0" role="button">Blog</a> <a href="https://github.com/xf-zhao/ISCM" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.48550/arXiv.2208.02680"></span> <span class="__dimensions_badge_embed__" data-doi="10.48550/arXiv.2208.02680" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Sound is one of the most informative and abundant modalities in the real world while being robust to sense without contacts by small and cheap sensors that can be placed on mobile devices. Although deep learning is capable of extracting information from multiple sensory inputs, there has been little use of sound for the control and learning of robotic actions. For unsupervised reinforcement learning, an agent is expected to actively collect experiences and jointly learn representations and policies in a self-supervised way. We build realistic robotic manipulation scenarios with physics-based sound simulation and propose the Intrinsic Sound Curiosity Module (ISCM). The ISCM provides feedback to a reinforcement learner to learn robust representations and to reward a more efficient exploration behavior. We perform experiments with sound enabled during pre-training and disabled during adaptation, and show that representations learned by ISCM outperform the ones by vision-only baselines and pre-trained policies can accelerate the learning process when applied to downstream tasks.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#318CE7"><a href="http://8.131.80.51/cscwd2021/index.html" rel="external nofollow noopener" target="_blank">CSCWD 2021</a></abbr></div> <div id="Wang21DensityWeighted" class="col-sm-8"> <div class="title">Density Weighted Diversity Based Query Strategy for Active Learning</div> <div class="author"> Tingting Wang,¬†<strong>Xufeng Zhao</strong>,¬†Qiujian Lv,¬†Bo Hu,¬†and¬†Degang Sun</div> <div class="periodical"> <em>In 2021 IEEE 24th International Conference on Computer Supported Cooperative Work in Design (CSCWD)</em>, May 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract ¬†<img src="/assets/img/arrowdown-icon.png" height="4"></a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.1109/CSCWD49262.2021.9437695"></span> <span class="__dimensions_badge_embed__" data-doi="10.1109/CSCWD49262.2021.9437695" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Deep learning has made remarkable achievements in various domains. Active learning, which aims to reduce the budget for training a machine-learning model, is especially useful for the Deep learning tasks with the demand of a large number of labeled samples. Unfortunately, our empirical study finds that many of the active learning heuristics are not effective when applied to Deep learning models in batch settings. To tackle these limitations, we propose a density weighted diversity based query strategy (DWDS), which makes use of the geometry of the samples. Within a limited labeling budget, DWDS enhances model performance by querying labels for the new training samples with the maximum informativeness and representativeness. Furthermore, we propose a beam-search based method to obtain a good approximation to the optimum of such samples. Our experiments show that DWDS outperforms existing algorithms in Deep learning tasks.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2018</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="Hu18ImpactCorrection" class="col-sm-8"> <div class="title">Impact and Correction of Phase Error in Ladar Signal on Synthetic Aperture Imaging</div> <div class="author"> Xuan Hu,¬†Daojing Li,¬†Tian He,¬†and¬†<strong>Xufeng Zhao</strong> </div> <div class="periodical"> <em>Infrared and Laser Engineering</em>, May 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract ¬†<img src="/assets/img/arrowdown-icon.png" height="4"></a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Specific to synthetic aperture ladar (SAL), the impact of signal phase error on synthetic aperture imaging was analyzed. Laser signal was modeled, the impact of laser signal coherence on SAL azimuthal resolution was analyzed, one solution by delaying the local oscillator signal was proposed. The impact of nonlinear distortion in LFM signal on range resolution was analyzed. To solve the problem of the random initial phase error introduced in the process of laser LFM signal modulation and amplification, one nonlinear distortion and phase error calibration correction method based on reference channel was proposed. Experiment and simulation results are shown.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="Zhao18MultipathClutter" class="col-sm-8"> <div class="title">Multi-path clutter suppression in passive radar reference channel based on digital TV signal</div> <div class="author"> <strong>Xufeng Zhao</strong>,¬†Daojing Li,¬†and¬†Xuan Hu</div> <div class="periodical"> <em>Journal of University of Chinese Academy of Sciences</em>, Jul 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract ¬†<img src="/assets/img/arrowdown-icon.png" height="4"></a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.7523/j.issn.2095-6134.2018.04.016"></span> <span class="__dimensions_badge_embed__" data-doi="10.7523/j.issn.2095-6134.2018.04.016" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Passive radar, which uses a third-party radiation source signal for mo...</p> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> ¬© Copyright 2023 Xufeng Zhao / ËµµÁª™Èîã. Last updated: November 08, 2023. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-1NETM326DJ"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-1NETM326DJ");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> <script data-name="BMC-Widget" data-cfasync="false" src="https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js" data-id="xufeng" data-description="Support me on Buy me a coffee!" data-message="" data-color="#FF813F" data-position="Right" data-x_margin="18" data-y_margin="18"></script> </html>
---
---

@string{aps = {American Physical Society,}}

@inproceedings{Zhao23ChatEnvironment,
  title = {Chat with the {{Environment}}: {{Interactive Multimodal Perception}} Using {{Large Language Models}}},
  shorttitle = {Chat with the {{Environment}}},
  booktitle = {2023 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Zhao, Xufeng and Li, Mengdi and Weber, Cornelius and Hafez, Muhammad Burhan and Wermter, Stefan},
  year = {2023},
  month = oct,
  number = {arXiv:2303.08268},
  eprint = {2303.08268},
  primaryclass = {cs, eess},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2303.08268},
  urldate = {2023-08-01},
  abstract = {Programming robot behaviour in a complex world faces challenges on multiple levels, from dextrous low-level skills to high-level planning and reasoning. Recent pre-trained Large Language Models (LLMs) have shown remarkable reasoning ability in zero-shot robotic planning. However, it remains challenging to ground LLMs in multimodal sensory input and continuous action output, while enabling a robot to interact with its environment and acquire novel information as its policies unfold. We develop a robot interaction scenario with a partially observable state, which necessitates a robot to decide on a range of epistemic actions in order to sample sensory information among multiple modalities, before being able to execute the task correctly. An interactive perception framework is therefore proposed with an LLM as its backbone, whose ability is exploited to instruct epistemic actions and to reason over the resulting multimodal sensations (vision, sound, haptics, proprioception), as well as to plan an entire task execution based on the interactively acquired information. Our study demonstrates that LLMs can provide high-level planning and reasoning skills and control interactive robot behaviour in a multimodal environment, while multimodal modules with the context of the environmental state help ground the LLMs and extend their processing ability.},
  archiveprefix = {arxiv},
  file = {/Users/xufeng/Library/CloudStorage/GoogleDrive-xfz.zhao@gmail.com/My Drive/Notes/PDFs/Zhao23ChatEnvironment.pdf},
  arxiv = {2303.08268},
  website = {https://matcha-model.github.io/},
  code = {https://github.com/xf-zhao/Matcha},
  preview = {https://xf-zhao.github.io/assets/img/publication_preview/Matcha_teaser.gif},
  abbr = {IROS 2023},
  selected = {true},
  poster = {https://matcha-model.github.io/img/matcha-poster.pdf},
}

@article{Hu18ImpactCorrection,
  title = {{Impact and Correction of Phase Error in Ladar Signal on Synthetic Aperture Imaging}},
  author = {Hu, Xuan and Li, Daojing and He, Tian and Zhao, Xufeng},
  year = {2018},
  journal = {Infrared and Laser Engineering},
  volume = {47},
  number = {3},
  pages = {306001--0306001},
  publisher = {{红外与激光工程}},
  abstract = {Specific to synthetic aperture ladar (SAL), the impact of signal phase error on synthetic aperture imaging was analyzed. Laser signal was modeled, the impact of laser signal coherence on SAL azimuthal resolution was analyzed, one solution by delaying the local oscillator signal was proposed. The impact of nonlinear distortion in LFM signal on range resolution was analyzed. To solve the problem of the random initial phase error introduced in the process of laser LFM signal modulation and amplification, one nonlinear distortion and phase error calibration correction method based on reference channel was proposed. Experiment and simulation results are shown.},
  langid = {cn},
}

@inproceedings{Li*23InternallyRewarded,
  title = {Internally {{Rewarded Reinforcement Learning}}},
  shorttitle = {{{ICML}}},
  booktitle = {40th {{International Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Li*, Mengdi and Zhao*, Xufeng and Lee, Jae Hee and Weber, Cornelius and Wermter, Stefan},
  year = {2023},
  month = jul,
  eprint = {2302.00270},
  primaryclass = {cs},
  urldate = {2023-07-28},
  abstract = {We study a class of reinforcement learning problems where the reward signals for policy learning are generated by a discriminator that is dependent on and jointly optimized with the policy. This interdependence between the policy and the discriminator leads to an unstable learning process because reward signals from an immature discriminator are noisy and impede policy learning, and conversely, an under-optimized policy impedes discriminator learning. We call this learning setting \textbackslash textit\{Internally Rewarded Reinforcement Learning\} (IRRL) as the reward is not provided directly by the environment but \textbackslash textit\{internally\} by the discriminator. In this paper, we formally formulate IRRL and present a class of problems that belong to IRRL. We theoretically derive and empirically analyze the effect of the reward function in IRRL and based on these analyses propose the clipped linear reward function. Experimental results show that the proposed reward function can consistently stabilize the training process by reducing the impact of reward noise, which leads to faster convergence and higher performance compared with baselines in diverse tasks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/xufeng/Library/CloudStorage/GoogleDrive-xfz.zhao@gmail.com/My Drive/Notes/PDFs/Li23InternallyRewarded.pdf},
  arxiv = {2302.00270},
  website = {https://ir-rl.github.io/},
  code = {https://github.com/mengdi-li/internally-rewarded-rl},
  preview = {https://xf-zhao.github.io/assets/img/publication_preview/IRRL_teaser.gif},
  abbr = {ICML 2023},
  selected = {true},
  poster = {https://ir-rl.github.io/img/ICML-poster.pdf},
}

@misc{Lu23CloserLook,
  title = {A {{Closer Look}} at {{Reward Decomposition}} for {{High-Level Robotic Explanations}}},
  author = {Lu, Wenhao and Zhao, Xufeng and Magg, Sven and Gromniak, Martin and Wermter, Stefan},
  year = {2023},
  month = Nov,
  number = {arXiv:2304.12958},
  eprint = {2304.12958},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2304.12958},
  urldate = {2023-08-21},
  abstract = {Explaining the behavior of intelligent agents such as robots to humans is challenging due to their incomprehensible proprioceptive states, variational intermediate goals, and resultant unpredictability. Moreover, one-step explanations for reinforcement learning agents can be ambiguous as they fail to account for the agent's future behavior at each transition, adding to the complexity of explaining robot actions. By leveraging abstracted actions that map to task-specific primitives, we avoid explanations on the movement level. Our proposed framework combines reward decomposition (RD) with abstracted action spaces into an explainable learning framework, allowing for non-ambiguous and high-level explanations based on object properties in the task. We demonstrate the effectiveness of our framework through quantitative and qualitative analysis of two robot scenarios, showcasing visual and textual explanations, from output artifacts of RD explanation, that are easy for humans to comprehend. Additionally, we demonstrate the versatility of integrating these artifacts with large language models for reasoning and interactive querying.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/xufeng/Library/CloudStorage/GoogleDrive-xfz.zhao@gmail.com/My Drive/Notes/PDFs/Lu23CloserLook.pdf},
  arxiv = {2304.12958},
  booktitle = {IEEE International Conference on Development and Learning 2023 (ICDL) },
  abbr = {IEEE ICDL 2023},
}

@inproceedings{Wang21DensityWeighted,
  title = {Density {{Weighted Diversity Based Query Strategy}} for {{Active Learning}}},
  booktitle = {2021 {{IEEE}} 24th {{International Conference}} on {{Computer Supported Cooperative Work}} in {{Design}} ({{CSCWD}})},
  author = {Wang, Tingting and Zhao, Xufeng and Lv, Qiujian and Hu, Bo and Sun, Degang},
  year = {2021},
  month = may,
  pages = {156--161},
  doi = {10.1109/CSCWD49262.2021.9437695},
  abstract = {Deep learning has made remarkable achievements in various domains. Active learning, which aims to reduce the budget for training a machine-learning model, is especially useful for the Deep learning tasks with the demand of a large number of labeled samples. Unfortunately, our empirical study finds that many of the active learning heuristics are not effective when applied to Deep learning models in batch settings. To tackle these limitations, we propose a density weighted diversity based query strategy (DWDS), which makes use of the geometry of the samples. Within a limited labeling budget, DWDS enhances model performance by querying labels for the new training samples with the maximum informativeness and representativeness. Furthermore, we propose a beam-search based method to obtain a good approximation to the optimum of such samples. Our experiments show that DWDS outperforms existing algorithms in Deep learning tasks.},
  keywords = {active learning,Approximation algorithms,Classification algorithms,Deep learning,density,diversity,Geometry,informativeness,Linear programming,representativeness,Search problems,Training},
  abbr = {CSCWD 2021},
}

@article{Zhao18MultipathClutter,
  title = {{Multi-path clutter suppression in passive radar reference channel based on digital TV signal}},
  author = {Zhao, Xufeng and Li, Daojing and Hu, Xuan},
  year = {2018},
  month = jul,
  journal = {Journal of University of Chinese Academy of Sciences},
  volume = {35},
  number = {4},
  pages = {529},
  issn = {2095-6134},
  doi = {10.7523/j.issn.2095-6134.2018.04.016},
  urldate = {2023-08-21},
  abstract = {Passive radar, which uses a third-party radiation source signal for mo...},
  langid = {cn},
  file = {/Users/xufeng/Library/CloudStorage/GoogleDrive-xfz.zhao@gmail.com/My Drive/Notes/PDFs/Xufeng18MultipathClutter.pdf}
}

@inproceedings{Zhao22ImpactMakes,
  title = {Impact {{Makes}} a {{Sound}} and {{Sound Makes}} an {{Impact}}: {{Sound Guides Representations}} and {{Explorations}}},
  booktitle = {2022 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Zhao, Xufeng and Weber, Cornelius and Hafez, Muhammad Burhan and Wermter, Stefan},
  year = {2022},
  month           = {Oct},
  pages = {2512--2518},
  publisher = {{IEEE}},
  doi = {10.48550/arXiv.2208.02680},
  abstract = {Sound is one of the most informative and abundant modalities in the real world while being robust to sense without contacts by small and cheap sensors that can be placed on mobile devices. Although deep learning is capable of extracting information from multiple sensory inputs, there has been little use of sound for the control and learning of robotic actions. For unsupervised reinforcement learning, an agent is expected to actively collect experiences and jointly learn representations and policies in a self-supervised way. We build realistic robotic manipulation scenarios with physics-based sound simulation and propose the Intrinsic Sound Curiosity Module (ISCM). The ISCM provides feedback to a reinforcement learner to learn robust representations and to reward a more efficient exploration behavior. We perform experiments with sound enabled during pre-training and disabled during adaptation, and show that representations learned by ISCM outperform the ones by vision-only baselines and pre-trained policies can accelerate the learning process when applied to downstream tasks.},
  file = {/Users/xufeng/Library/CloudStorage/GoogleDrive-xfz.zhao@gmail.com/My Drive/Notes/PDFs/Zhao22ImpactMakes.pdf},
  arxiv = {2208.02680},
  preview = {ISCM_teaser.jpg},
  abbr = {IROS 2022},
  selected = {true},
  code = {https://github.com/xf-zhao/ISCM},
  blog = {news/IROS2022/},
}

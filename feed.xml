<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://xf-zhao.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://xf-zhao.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-12-02T10:25:11+00:00</updated><id>https://xf-zhao.github.io/feed.xml</id><title type="html">blank</title><subtitle>üïò 3rd year Ph.D student in University of Hamburg; Researching on reinforcement learning (RL), robotic, and large language models (LLMs). </subtitle><entry><title type="html">Elo Rating, Logistic Distribution, and Logistic Regression</title><link href="https://xf-zhao.github.io/post/2023/elo-rating/" rel="alternate" type="text/html" title="Elo Rating, Logistic Distribution, and Logistic Regression"/><published>2023-11-05T00:00:00+00:00</published><updated>2023-11-05T00:00:00+00:00</updated><id>https://xf-zhao.github.io/post/2023/elo-rating</id><content type="html" xml:base="https://xf-zhao.github.io/post/2023/elo-rating/"><![CDATA[<p><strong>Long story short, this post tries to explain Elo rating with a probabilistic perspective, especially the relation with a logistic distribution that many of the existing blogs failed to do.</strong></p> <p>I am currently reading papers about large language models alignment tuning <sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>, e.g. reinforcement learning from human feedback (RLHF) used for ChatGPT. One of many kinds of human feedback is ‚Äúranking-based approach‚Äù <sup id="fnref:1:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>, and then I ran into a related paper by DeepMind that uses Elo rating for the feedback <sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>. Elo rating was initially created for assessing chess palyers‚Äô skill but later widely applied for online games such as League of Legends ranking <sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>.</p> <p>There already exist many webpages, including <a href="https://en.wikipedia.org/wiki/Elo_rating_system">this Elo rating Wikipedia page</a>, that try to explain Elo rating with mathematical formulas. But, as far as I can go, they fail to explain clearly to beginners, for example, who is logistically distributed, and how to derive the final logistic regression.</p> <h2 id="elo-rating">Elo Rating</h2> <p>The core idea of Elo rating system is to ‚Äúlearn‚Äù a score, according to their previous comptetion performance, to reveal each player‚Äôs skill ability. As a result, the win/loss probability of any two players can be well predicted by comparing the score differnce <sup id="fnref:6" role="doc-noteref"><a href="#fn:6" class="footnote" rel="footnote">4</a></sup> <sup id="fnref:7" role="doc-noteref"><a href="#fn:7" class="footnote" rel="footnote">5</a></sup>. For a win/loss games (i.e. no draw), supposing players A, B are assigned with score \(R_A\) and \(R_B\) respectively, the probability of player A to win is calculated by Elo rating as</p> \[\text{Pr}(\text{A wins}) = \frac{1}{1+10^{-r_{AB}/400}},\] <p>where \(r_{AB} = R_A - R_B\) is the score difference between player A and B. Apprently, this equation is in a logistic function <sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">6</a></sup>. And \(R_A\) and \(R_B\) can be updated with optimization methods, e.g. SGD, accordingly <sup id="fnref:3:1" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>. This updating procedure ends up with formula</p> \[R_A \leftarrow R_A + K [S_A - \text{Pr}(\text{A wins})],\] <p>where \(S_A\) indicates whether player A wins (1) or not (0) (and also similar for the update of player B).</p> <p>In machine learning, a <em>softmax function</em> links non-bounded values with probability: \(p_i = \frac{e^{x_i}}{\sum e^{x_i}}\). A special case when there are only two variables \(p_a = \frac{e^a}{e^a + e^b} = \frac{1}{1 + e^{b-a}}\), which can be usually denoted using <em>sigmoid function</em> \(\sigma(t) = \frac{1}{1+e^{-t}}\) (in this case let \(t = a-b\), in which only the difference between \(a\) and \(b\) matters).</p> <p>Note that the this process is a litter different from machine learning with a batch size because the competetions are streaming data to process, i.e. update parameters <em>with given sample order</em>. If we have directly optimize on the whole data instead of streaming update, the question can be now described as: <em>‚ÄúHow to map the win/loss frequency (or probability) of two players with their skill score difference?‚Äù</em> This reduces the degree of freedom of variable to 1 (the difference instead of absolute value of two scores), leading to the fitting problem of \(p = \frac{1}{1+e^{-t/s}}\), where \(p\) is the probability of, for example, ‚ÄúA wins‚Äù and \(s\) is a scaling factor. Solving this equation leads to \(t = s \ln \frac{p}{1-p}\). In Elo rating system, \(s = 400\), and the base is chosen as \(10\) instead of \(e\) for human-friendly interpretation. Take a small example of the formula: supposing that player A is 400 score less than player B, the log-odds <sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">7</a></sup> \(\log \frac{p}{1-p} = -1\), and then we get \(p = \frac{1}{11}\) (or the odd \(\frac{p}{1-p} = 1:10\)). Intuitively, a -400 rating difference means that player A only has a chance of \(\frac{1}{11}\) to win (learned according to statistics).</p> <h2 id="logistic-distribution-behind-elo-rating">Logistic Distribution behind Elo Rating</h2> <p>So far the calculation of Elo rating scores and the link to probability meaning is introduced. The assumption is that player performance difference can be modeled vis logistic regression. A higher rating score generally means a better player. But because of the randomness in competetions, it happens that a ‚Äúgenerally better player‚Äù loses. There are claims <sup id="fnref:3:2" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup> <sup id="fnref:6:1" role="doc-noteref"><a href="#fn:6" class="footnote" rel="footnote">4</a></sup> <sup id="fnref:9" role="doc-noteref"><a href="#fn:9" class="footnote" rel="footnote">8</a></sup> pointing out that player‚Äôs <em>performance</em> is rather logistically distributed instead of normally. This question confused me a lot:</p> <p><strong>‚ÄúWhat exactly is logistically distributed such that the Elo rating‚Äôs logistic regression form can be derived?‚Äù</strong></p> <p>Let me start by quoting a <a href="https://www.cantorsparadise.com/the-mathematics-of-elo-ratings-b6bfc9ca1dba">this blog</a> <sup id="fnref:6:2" role="doc-noteref"><a href="#fn:6" class="footnote" rel="footnote">4</a></sup>,</p> <blockquote> <p>the system assumed that players‚Äô expected scores conform to a normal distribution. Later, both the USCF and FIDE altered their systems when it was found their empirics suggested that performance in chess more resembles a logistic distribution (heavier tails/higher probability of extreme outcomes).</p> </blockquote> <h3 id="1-player-absolute-performance">1. Player Absolute Performance</h3> <p>It seems that the intuitive purpose of changing model is to describe (better model fitting) how often a <em>‚Äúblack swan event‚Äù</em> ‚Äî a low-score player beats a high-score player ‚Äî happens. However, recall that the pdf of logistic distribution <sup id="fnref:10" role="doc-noteref"><a href="#fn:10" class="footnote" rel="footnote">9</a></sup> is defined as</p> \[f(t) = \frac{u}{s (1+u)^2}, u = e^{-(t-\mu)/s}.\] <p>Its cdf is in a logistic function form: \(F(t) = \frac{1}{1+e^{-(t-\mu)/s}}\).</p> <p>The actually performance of player A and B is defined as random variable \(\alpha_A\) and \(\alpha_B\) respectively.</p> <p>\begin{align} \label{eq:awins} \text{Pr}(\text{A wins}) = \text{Pr}(\alpha_A &gt; \alpha_B) = \int_{-\infty}^{+\infty} \text{Pr}(\alpha_A = a) \cdot \text{Pr}(\alpha_B&lt;a) da = \int_{-\infty}^{+\infty} f(a) da \int_{-\infty}^{a} f(b) db. \end{align}</p> <p>In Eq.\eqref{eq:awins}, \(f(\cdot)\) is the pdf of the players‚Äô performance. This equation relates to compute the pdf of a new random variable \(\alpha_C = \alpha_A - \alpha_B\). If both \(\alpha_A\) and \(\alpha_B\) conform normal distritribution, the resultant rv \(\alpha_C\) will also follow a normal distribution <sup id="fnref:11" role="doc-noteref"><a href="#fn:11" class="footnote" rel="footnote">10</a></sup>. However, this is not the case for logistic distribution <sup id="fnref:12" role="doc-noteref"><a href="#fn:12" class="footnote" rel="footnote">11</a></sup> <sup id="fnref:13" role="doc-noteref"><a href="#fn:13" class="footnote" rel="footnote">12</a></sup>. So the answer to the question is <strong>NOT</strong> exactly the player‚Äôs perfomance, i.e. \(\alpha\). But according to the close curve look of the norm and logistic distribution, we can assume that ‚Äúthe sum is approximately logistic‚Äù <sup id="fnref:7:1" role="doc-noteref"><a href="#fn:7" class="footnote" rel="footnote">5</a></sup> <sup id="fnref:12:1" role="doc-noteref"><a href="#fn:12" class="footnote" rel="footnote">11</a></sup>. By this approximation, we can roughly say that it is the player‚Äôs performance that conforms logistic distribution.</p> <h3 id="2-logistically-distributed-variable">2. Logistically Distributed Variable</h3> <p>If we go precise, the answer should be</p> <blockquote> <p>Elo assumes errors are distributed according to a logistic distribution <sup id="fnref:14" role="doc-noteref"><a href="#fn:14" class="footnote" rel="footnote">13</a></sup></p> </blockquote> <p>or more accurately</p> <blockquote> <p>the difference in the random components of performance is logistically distributed <sup id="fnref:9:1" role="doc-noteref"><a href="#fn:9" class="footnote" rel="footnote">8</a></sup></p> </blockquote> <p>as is straightfoward from the equations. Nevertheless, the <em>difference in randomness</em> is no longer intuitive for human to understand anymore. I would like to give an intuition that ‚Äúthe status of players to be both better or worse (than his normal self) is generally the same, e.g. high chance that both perfom worse due to the weather.</p> <h2 id="whats-next">What‚Äôs Next</h2> <p>Explanation of the usage in alignment tuning LLMs.</p> <hr/> <h2 id="reference-and-footnote">Reference and Footnote</h2> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1" role="doc-endnote"> <p>W. X. Zhao et al., ‚ÄúA Survey of Large Language Models.‚Äù arXiv, Sep. 11, 2023. Accessed: Oct. 24, 2023. [Online]. Available: <a href="http://arxiv.org/abs/2303.18223">http://arxiv.org/abs/2303.18223</a>¬†<a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a>¬†<a href="#fnref:1:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p> </li> <li id="fn:2" role="doc-endnote"> <p>A. Glaese et al., ‚ÄúImproving alignment of dialogue agents via targeted human judgements.‚Äù arXiv, Sep. 28, 2022. <a href="https://arxiv.org/abs/2209.14375">https://arxiv.org/abs/2209.14375</a>.¬†<a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:3" role="doc-endnote"> <p><a href="https://leagueoflegends.fandom.com/wiki/Elo_rating_system">League of Lengends wiki: Elo Rating</a>¬†<a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a>¬†<a href="#fnref:3:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a>¬†<a href="#fnref:3:2" class="reversefootnote" role="doc-backlink">&#8617;<sup>3</sup></a></p> </li> <li id="fn:6" role="doc-endnote"> <p><a href="https://www.cantorsparadise.com/the-mathematics-of-elo-ratings-b6bfc9ca1dba">The Mathematics of Elo Ratings: Calculating the relative skill of players in zero-sum games</a>¬†<a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a>¬†<a href="#fnref:6:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a>¬†<a href="#fnref:6:2" class="reversefootnote" role="doc-backlink">&#8617;<sup>3</sup></a></p> </li> <li id="fn:7" role="doc-endnote"> <p><a href="https://www.kaggle.com/code/dancumming/math3311-group4">Kaggle MATH3311-Group4: Logistic Distribution on Elo Chess Data</a>¬†<a href="#fnref:7" class="reversefootnote" role="doc-backlink">&#8617;</a>¬†<a href="#fnref:7:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p> </li> <li id="fn:4" role="doc-endnote"> <p>The logistic function is modeled as \(p(x) = \frac{1}{1 + e^{\frac{x-\mu}{s}}}\), where \(\mu\) and \(s\) controls the curve shifting and steep respectively (see <a href="https://en.wikipedia.org/wiki/Logistic_regression">wiki</a> for more). And correspondingly, logistic regression is actually classification model that tries to learn parameters (normally in the format of \(p(x) = \frac{1}{1+e^{\mathbf{w}^\intercal x}}\)) of the logistic function model to correctly (in probability) classify the input into two different categories.¬†<a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:5" role="doc-endnote"> <p><a href="https://en.wikipedia.org/wiki/Logit">https://en.wikipedia.org/wiki/Logit</a>¬†<a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:9" role="doc-endnote"> <p><a href="https://nicidob.github.io/nba_elo/">FiveThirtyEight‚Äôs Elo Ratings and Logistic Regression</a>¬†<a href="#fnref:9" class="reversefootnote" role="doc-backlink">&#8617;</a>¬†<a href="#fnref:9:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p> </li> <li id="fn:10" role="doc-endnote"> <p><a href="https://en.wikipedia.org/wiki/Logistic_distribution">Wikipedia: Logistic distribution</a>¬†<a href="#fnref:10" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:11" role="doc-endnote"> <p><a href="https://srabbani.com/bivariate.pdf">Proof that the Difference of Two Jointly Distributed Normal Random Variables is Normal</a>¬†<a href="#fnref:11" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:12" role="doc-endnote"> <p><a href="https://stackoverflow.com/questions/2823468/how-do-i-add-variables-with-logistic-distributions#:~:text=The%20sum%20of%20two%20logistic%20random%20variables%20does%20not%20have,normal%20random%20variables%20is%20normal.">stackoverflow: how do i add variables with logistic distributions?</a>¬†<a href="#fnref:12" class="reversefootnote" role="doc-backlink">&#8617;</a>¬†<a href="#fnref:12:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p> </li> <li id="fn:13" role="doc-endnote"> <p><a href="https://mathoverflow.net/questions/157569/explicit-expressions-of-the-distribution-of-sums-of-i-i-d-logistic-random-varia">mathoverflow: explicit expressions of the distribution of sums of i.i.d. logistic random variables</a>¬†<a href="#fnref:13" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:14" role="doc-endnote"> <p><a href="https://stats.stackexchange.com/questions/422243/what-are-the-elo-formulas-when-assuming-performance-to-be-logistically-distribut">stackexchange: What are the Elo formulas when assuming performance to be logistically distributed?</a>¬†<a href="#fnref:14" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="work"/><category term="rating"/><category term="probability"/><category term="large language models"/><summary type="html"><![CDATA[A probabilistic explanation of Elo rating approach]]></summary></entry></feed>